{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python37564bitprevvenvaaf3ad2477cc402fa47ee19ffa39844b",
   "display_name": "Python 3.7.5 64-bit ('prev': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа №4. Реализация приложения по распознаванию номеров домов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow и tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import os\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tarfile\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "l2_regularization = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1.\n",
    "Реализуйте глубокую нейронную сеть (полносвязную или сверточную) и обучите ее на синтетических данных (например, наборы MNIST (http://yann.lecun.com/exdb/mnist/) или notMNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "\n",
    "def extract_dataset():\n",
    "    with open('../data/notMNIST_sanit.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # reshape dataset because of error:\n",
    "    # ValueError: Error when checking input: expected conv2d_input to have 4 dimensions,\n",
    "    # but got array with shape (200000, 28, 28)\n",
    "    for key, dataset in data.items():\n",
    "        data[key] = dataset.reshape(*dataset.shape, 1)\n",
    "    return data\n",
    "\n",
    "def image_name(index):\n",
    "  return chr(ord('A') + index)\n",
    "\n",
    "dataset = extract_dataset()\n",
    "train_images = dataset['train_dataset']\n",
    "train_labels = dataset['train_labels']\n",
    "valid_images = dataset['valid_dataset']\n",
    "valid_labels = dataset['valid_labels']\n",
    "test_images = dataset['test_dataset']\n",
    "test_labels = dataset['test_labels']\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 28, 28, 16)        416       \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 14, 14, 16)        0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 14, 14, 16)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 10, 10, 32)        12832     \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 5, 5, 32)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               96120     \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 120)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 84)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 120,382\nTrainable params: 120,382\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 200000 samples, validate on 16955 samples\nEpoch 1/10\n200000/200000 [==============================] - 150s 751us/sample - loss: 0.7281 - accuracy: 0.7970 - val_loss: 0.4962 - val_accuracy: 0.8678\nEpoch 2/10\n200000/200000 [==============================] - 142s 708us/sample - loss: 0.5881 - accuracy: 0.8477 - val_loss: 0.4796 - val_accuracy: 0.8750\nEpoch 3/10\n200000/200000 [==============================] - 161s 805us/sample - loss: 0.5736 - accuracy: 0.8528 - val_loss: 0.4658 - val_accuracy: 0.8801\nEpoch 4/10\n200000/200000 [==============================] - 138s 692us/sample - loss: 0.5639 - accuracy: 0.8558 - val_loss: 0.4653 - val_accuracy: 0.8797\nEpoch 5/10\n200000/200000 [==============================] - 107s 535us/sample - loss: 0.5591 - accuracy: 0.8577 - val_loss: 0.4571 - val_accuracy: 0.8819\nEpoch 6/10\n200000/200000 [==============================] - 107s 535us/sample - loss: 0.5562 - accuracy: 0.8584 - val_loss: 0.4516 - val_accuracy: 0.8848\nEpoch 7/10\n200000/200000 [==============================] - 108s 540us/sample - loss: 0.5515 - accuracy: 0.8602 - val_loss: 0.4491 - val_accuracy: 0.8846\nEpoch 8/10\n200000/200000 [==============================] - 107s 537us/sample - loss: 0.5479 - accuracy: 0.8608 - val_loss: 0.4524 - val_accuracy: 0.8819\nEpoch 9/10\n200000/200000 [==============================] - 108s 538us/sample - loss: 0.5476 - accuracy: 0.8605 - val_loss: 0.4459 - val_accuracy: 0.8853\nEpoch 10/10\n200000/200000 [==============================] - 108s 540us/sample - loss: 0.5451 - accuracy: 0.8613 - val_loss: 0.4489 - val_accuracy: 0.8851\n8720/8720 - 1s - loss: 0.2603 - accuracy: 0.9462\n\nТочность на проверочных данных: 0.94621557\n"
    }
   ],
   "source": [
    "numbers_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=[5,5], padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=[2,2], strides=2),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=[5,5], padding='valid', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=[2,2], strides=2),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=120, activation='relu', kernel_regularizer=regularizers.l2(l2_regularization)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=84, activation='relu', kernel_regularizer=regularizers.l2(l2_regularization)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "numbers_model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "numbers_model.summary()\n",
    "\n",
    "numbers_model_history = numbers_model.fit(train_images,\n",
    "                                      train_labels,\n",
    "                                      epochs=10,\n",
    "                                      validation_data=(valid_images, valid_labels))\n",
    "\n",
    "test_loss, test_acc = numbers_model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nТочность на проверочных данных:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "После уточнения модели на синтетических данных попробуйте обучить ее на реальных данных (набор Google Street View). Что изменилось в модели?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data\n",
    "\n",
    "dataset_paths = [('test', \"../data/housenames/test.tar.gz\"),\n",
    "                 ('train', \"../data/housenames/train.tar.gz\")]\n",
    "for name, path in dataset_paths:\n",
    "    if not os.path.exists('../data/housenames/' + name):\n",
    "        print(name)\n",
    "        tf = tarfile.open(path)\n",
    "        files = tf.extractall('../data/housenames')\n",
    "        tf.close()\n",
    "\n",
    "houses_train_dataset= scipy.io.loadmat(os.path.join('../data/housenames/', 'train_32x32.mat'))\n",
    "houses_test_dataset = scipy.io.loadmat(os.path.join('../data/housenames/', 'test_32x32.mat'))\n",
    "\n",
    "\n",
    "# Fomratting of data\n",
    "\n",
    "# 1) move axis to be like this: (num of imagex, h, w, dimensions)\n",
    "houses_train_dataset['X'] = np.moveaxis(houses_train_dataset['X'], 3, 0)\n",
    "houses_test_dataset['X'] = np.moveaxis(houses_test_dataset['X'], 3, 0)\n",
    "\n",
    "# 2) subtract 1 from labels because they start from 1\n",
    "houses_train_dataset['y'] = houses_train_dataset['y'] - 1\n",
    "houses_test_dataset['y'] = houses_test_dataset['y'] - 1\n",
    "\n",
    "\n",
    "\n",
    "houses_train_images, houses_valid_images, houses_train_labels, houses_valid_labels = train_test_split(\n",
    "    houses_train_dataset['X'],\n",
    "    houses_train_dataset['y']\n",
    ")\n",
    "\n",
    "houses_test_images = houses_test_dataset['X']\n",
    "houses_test_labels = houses_test_dataset['y']\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 28, 28, 16)        416       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 14, 14, 16)        0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 14, 14, 16)        0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 10, 10, 32)        12832     \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 5, 5, 32)          0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 800)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 120)               96120     \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 120)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 84)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 120,382\nTrainable params: 120,382\nNon-trainable params: 0\n_________________________________________________________________\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_6_input to have shape (28, 28, 1) but got array with shape (32, 32, 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0f62d5797b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0mhouses_train_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                           validation_data=(houses_valid_images, houses_valid_labels))\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhouses_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhouses_test_images\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mhouses_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/study/mo/prev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_6_input to have shape (28, 28, 1) but got array with shape (32, 32, 3)"
     ]
    }
   ],
   "source": [
    "houses_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=[5,5], padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.MaxPooling2D(pool_size=[2,2], strides=2),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=[5,5], padding='valid', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=[2,2], strides=2),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=120, activation='relu', kernel_regularizer=regularizers.l2(l2_regularization)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=84, activation='relu', kernel_regularizer=regularizers.l2(l2_regularization)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "houses_model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "houses_model.summary()\n",
    "\n",
    "houses_model_history = houses_model.fit(houses_train_images,\n",
    "                          houses_train_labels,\n",
    "                          epochs=10,\n",
    "                          validation_data=(houses_valid_images, houses_valid_labels))\n",
    "\n",
    "test_loss, test_acc = houses_model.evaluate(houses_test_images,  houses_test_labels, verbose=2)"
   ]
  }
 ]
}